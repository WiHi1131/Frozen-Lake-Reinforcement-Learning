{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3c6784-a406-4c7b-b91d-e62ea99a9615",
   "metadata": {},
   "source": [
    "# Frozen Lake Reinforcement Learning Project\n",
    "\n",
    "William Hinkley \n",
    "\n",
    "CSPB 3202 \n",
    "\n",
    "Fall 2023\n",
    "\n",
    "Github Repository Link: https://github.com/WiHi1131/Frozen-Lake-Reinforcement-Learning\n",
    "\n",
    "YouTube Link: https://youtu.be/kjfbU22-EUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb68de-f7d5-454d-a84b-bc5978abc940",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This report presents a comprehensive study conducted in the context of Artificial Intelligence and intelligent agents, focusing on the application of reinforcement learning (RL) techniques. The core of the project revolves around the implementation and analysis of various RL models tasked with navigating the challenging FrozenLake environment from OpenAI's Gymnasium.\n",
    "\n",
    "## Reinforcement Learning Models\n",
    "\n",
    "Three distinct RL models were explored:\n",
    "\n",
    "1. **Value Iteration**: A classic RL algorithm that involves updating the value function iteratively to converge to the optimal policy.\n",
    "2. **Q-Learning**: A model-free, off-policy algorithm, which estimates the value of action-state pairs by using a Q-table and updates these values based on the Bellman equation.\n",
    "3. **Deep Q-Learning Network (DQN)**: An advanced approach that integrates deep learning with Q-learning, employing neural networks to approximate the Q-value function.\n",
    "\n",
    "## The FrozenLake Environment\n",
    "\n",
    "### Environment Description\n",
    "\n",
    "- **Theme**: Players navigate across a frozen lake, aiming to reach a goal without falling into holes.\n",
    "- **Slippery Terrain**: The lake's slippery nature adds randomness to movement, causing occasional deviations from the intended direction.\n",
    "- **Map Image**: \n",
    "\n",
    "![Frozen Lake 4x4 Map](./images/frozen_lake_4x4.PNG)\n",
    "\n",
    "### Specifications\n",
    "\n",
    "- **Action Space**: Discrete(4) - {0: Left, 1: Down, 2: Right, 3: Up}\n",
    "- **Observation Space**: Discrete(16) for the 4x4 map, representing the player's current position.\n",
    "- **Starting State**: Player begins at [0,0].\n",
    "- **Rewards**: +1 for reaching the goal, 0 otherwise.\n",
    "- **Episode Termination**: Occurs upon falling into a hole or reaching the goal.\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "- **Default Map (4x4)** (see image above)\n",
    "- **Large Map (8x8)**: \n",
    "\n",
    "![Frozen Lake 8x8 Map](./images/frozen_lake_8x8.PNG)\n",
    "- **Slippery Condition**: For each map, the is_slippery condition is set to true. This means that the player only moves in the intended direction 1/3 of the time, otherwise the player will move \"in either perpendicular direction with equal probability of 1/3 in both directions.For example, if action is left and is_slippery is True, then: P(move left)=1/3, P(move up)=1/3, P(move down)=1/3\" (https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "This project aims to evaluate and compare the effectiveness of these RL models in mastering the FrozenLake game, offering insights into their learning capabilities and adaptability in a stochastic environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511e13a-b53b-4157-82a0-711df337335e",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "## Model Selection Rationale\n",
    "\n",
    "For this project, I chose three models: Value Iteration, Q-Learning, and Deep Q-Learning Network (DQN), each representing a unique approach in the spectrum of reinforcement learning (RL).\n",
    "\n",
    "1. **Value Iteration**: Selected for its foundational significance in RL, providing a clear understanding of policy and value function interactions in deterministic environments.\n",
    "2. **Q-Learning**: As a model-free algorithm, it offers insights into learning dynamics without requiring a model of the environment, making it suitable for stochastic settings like FrozenLake.\n",
    "3. **Deep Q-Learning Network (DQN)**: Chosen for its ability to handle high-dimensional observation spaces and to demonstrate the integration of deep learning with traditional RL methods.\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "### Development Environment\n",
    "\n",
    "- **IDE**: Visual Studio Code was used for its robust Python support, ease of managing multiple scripts, and integrated debugging tools.\n",
    "- **Dependency Management**: Necessary libraries, including Gymnasium and Box2D/Toy Text, were installed to facilitate environment setup and agent interactions.\n",
    "- **Local Execution**: Scripts were run locally to manage computational resources effectively while ensuring real-time visualization and interaction with the environment.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "- A key aspect of the project was the visualization of the agent's performance in the environment. The Box2D package provided a simple yet effective way to render the game, allowing for immediate feedback on the agent's behavior and strategy effectiveness.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- **Average Reward/Score**: The primary metric for evaluating model performance was the average reward or score obtained over time across all training episodes. This metric was chosen for its direct correlation with the agent's ability to navigate the environment successfully, reflecting both the efficiency and effectiveness of the learning process.\n",
    "\n",
    "\n",
    "## Troubleshooting and Optimization\n",
    "\n",
    "- **Rendering and Computational Complexity**: One of the challenges was finding an environment setup that allowed easy rendering of the game for visualization purposes while balancing the computational load. This balance was crucial for running the models on a local machine without compromising performance.\n",
    "- **Environment Compatibility**: Ensuring compatibility with the chosen RL environments and the visualization tools was a significant part of the setup process. This required testing different versions of libraries and adjusting configurations to achieve optimal functionality, as well as downloading swigwin and Visual Studio to get the visualization of the environment to work. \n",
    "\n",
    "## Testing on Different Environment Scales\n",
    "\n",
    "### Simple 4x4 Version\n",
    "\n",
    "- Initially, the models were tested on the simpler 4x4 version of the FrozenLake environment. This provided a controlled setting to fine-tune the models and understand their learning dynamics in a relatively straightforward context.\n",
    "\n",
    "### Complex 8x8 Version\n",
    "\n",
    "- To assess the scalability and adaptability of the models, they were subsequently tested on the more challenging 8x8 version of FrozenLake. This larger and more complex environment offered a better understanding of how the models perform under increased environmental complexity and uncertainty.\n",
    "- The performance in the 8x8 environment was particularly crucial in evaluating the models' ability to generalize their learning strategies to larger state spaces and more intricate navigation challenges.\n",
    "\n",
    "### Code: \n",
    "\n",
    "- The following code snippets show all the python scripts used in my project, with comments added for readability. Each snippet is titled with the title of the file (all files can also be found in the github repository for my project). These snippets are not designed to be run within this Jupyter notebook, and were run in VS Code on my local machine. \n",
    "- Note: Some of the code to implement the q-learning agent file was adapted from the following online source: https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake_unslippery%20(Deterministic%20version).ipynb\n",
    "- Note: test.py is not described by any snippets below - this was merely a testing script to ensure the environment could be visualized properly for my reference. \n",
    "\n",
    "#### value_iteration_policy.py: \n",
    "- this script defines my value iteration policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329adbe-50af-4a97-96cb-90ce3bf3ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Importing the Gym library for creating and managing environments\n",
    "import numpy as np  # Importing NumPy for numerical operations\n",
    "\n",
    "# Define the function for running value iteration\n",
    "def run_value_iteration(env, discount_factor=0.9, theta=1e-4, print_iterations=[100, 1000]):\n",
    "    value_table = np.zeros(env.observation_space.n)  # Initialize the value table with zeros\n",
    "    policy_table = np.zeros(env.observation_space.n, dtype=int)  # Initialize the policy table with zeros\n",
    "    iteration = 0  # Counter for iterations\n",
    "\n",
    "    while True:  # Start of the value iteration loop\n",
    "        delta = 0  # Initialize the delta, which tracks the change in value\n",
    "        iteration += 1  # Increment the iteration count\n",
    "\n",
    "        # Loop over all states in the environment\n",
    "        for state in range(env.observation_space.n):\n",
    "            v = value_table[state]  # Store the current value of the state\n",
    "            # Update the value of the state based on the Bellman equation\n",
    "            value_table[state] = max(sum(prob * (reward + discount_factor * value_table[next_state])\n",
    "                                        for prob, next_state, reward, _ in env.P[state][action])\n",
    "                                    for action in range(env.action_space.n))\n",
    "            # Update delta with the maximum change observed in the value table\n",
    "            delta = max(delta, abs(v - value_table[state]))\n",
    "\n",
    "        # Print the value table at specified iterations\n",
    "        if iteration in print_iterations:\n",
    "            print(f\"Value Table after {iteration} iterations:\")\n",
    "            print(value_table)\n",
    "\n",
    "        # Check for convergence, break if the change is below the threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Policy extraction loop\n",
    "    for state in range(env.observation_space.n):\n",
    "        # For each state, find the best action by looking at the future rewards\n",
    "        policy_table[state] = np.argmax([sum(prob * (reward + discount_factor * value_table[next_state])\n",
    "                                             for prob, next_state, reward, _ in env.P[state][action])\n",
    "                                         for action in range(env.action_space.n)])\n",
    "\n",
    "    return policy_table  # Return the final policy table\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('FrozenLake-v1', render_mode=\"human\")  # Create the FrozenLake environment\n",
    "    policy = run_value_iteration(env)  # Run value iteration on the environment\n",
    "    env.close()  # Close the environment after running value iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398fa7b-f984-4b68-b1d2-9a906d4cb23d",
   "metadata": {},
   "source": [
    "#### test_value_iteration_policy.py \n",
    "- this tests and renders the environment where an agent uses the value iteration policy defined in value_iteration_policy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e050cd-3558-40db-a33b-3780779b94d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Importing the Gym library for creating and managing environments\n",
    "from value_iteration_policy import run_value_iteration  # Importing the value iteration function\n",
    "\n",
    "# Function to test the policy in the environment\n",
    "def test_policy(env, policy, total_episodes=100):\n",
    "    total_rewards = 0  # Initialize total rewards\n",
    "\n",
    "    # Run the policy for a specified number of episodes\n",
    "    for episode in range(total_episodes):\n",
    "        observation, info = env.reset()  # Reset the environment at the start of each episode\n",
    "        episode_reward = 0  # Initialize reward for this episode\n",
    "\n",
    "        # Run the episode for a maximum of 99 steps\n",
    "        for _ in range(99):\n",
    "            action = policy[observation]  # Select an action based on the policy\n",
    "            # Perform the action in the environment and get the next state and reward\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward  # Accumulate reward\n",
    "\n",
    "            # Break the loop if the episode is terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        total_rewards += episode_reward  # Add episode reward to total rewards\n",
    "\n",
    "    average_reward = total_rewards / total_episodes  # Calculate the average reward\n",
    "    return average_reward  # Return the average reward\n",
    "\n",
    "# Function to render and demonstrate the policy in the environment\n",
    "def render_policy(env, policy, total_episodes=5):\n",
    "    for episode in range(total_episodes):  # Loop for a specified number of episodes\n",
    "        observation, info = env.reset()  # Reset the environment at the start of each episode\n",
    "\n",
    "        # Run the episode for a maximum of 99 steps\n",
    "        for step in range(99):\n",
    "            env.render()  # Render the current state of the environment\n",
    "            action = policy[observation]  # Select an action based on the policy\n",
    "            # Perform the action in the environment and get the next state and reward\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Break the loop if the episode is terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                print(\"****************************************************\")\n",
    "                print(f\"EPISODE {episode + 1}\")\n",
    "                print(\"Number of steps:\", step)\n",
    "                break\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('FrozenLake-v1', render_mode=None)  # Create environment without rendering for training\n",
    "    policy = run_value_iteration(env)  # Run value iteration to get the policy\n",
    "\n",
    "    # Test the policy and print the average reward\n",
    "    average_reward = test_policy(env, policy, 100)\n",
    "    print(\"Average Score over time: \" + str(average_reward))\n",
    "\n",
    "    # Create environment with rendering for demonstration\n",
    "    env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "    render_policy(env, policy, 5)  # Render and demonstrate the policy\n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb30400-e3b1-41e0-91c3-320337d1e0c2",
   "metadata": {},
   "source": [
    "#### test_vi_frozen_8x8.py \n",
    "- this script tests a value iteration policy on an 8x8 version of Frozen Lake: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60b045-0363-4190-a229-d95192c4be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Importing the Gym library for creating and managing environments\n",
    "from value_iteration_policy import run_value_iteration  # Importing the value iteration function\n",
    "\n",
    "# Function to test the policy in the environment\n",
    "def test_policy(env, policy, total_episodes=100):\n",
    "    total_rewards = 0  # Initialize total rewards\n",
    "\n",
    "    # Run the policy for a specified number of episodes\n",
    "    for episode in range(total_episodes):\n",
    "        observation, info = env.reset()  # Reset the environment at the start of each episode\n",
    "        episode_reward = 0  # Initialize reward for this episode\n",
    "\n",
    "        # Run the episode for a maximum of 199 steps\n",
    "        for _ in range(199):\n",
    "            action = policy[observation]  # Select an action based on the policy\n",
    "            # Perform the action in the environment and get the next state and reward\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward  # Accumulate reward\n",
    "\n",
    "            # Break the loop if the episode is terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        total_rewards += episode_reward  # Add episode reward to total rewards\n",
    "\n",
    "    average_reward = total_rewards / total_episodes  # Calculate the average reward\n",
    "    return average_reward  # Return the average reward\n",
    "\n",
    "# Function to render and demonstrate the policy in the environment\n",
    "def render_policy(env, policy, total_episodes=5):\n",
    "    for episode in range(total_episodes):  # Loop for a specified number of episodes\n",
    "        observation, info = env.reset()  # Reset the environment at the start of each episode\n",
    "\n",
    "        # Run the episode for a maximum of 199 steps\n",
    "        for step in range(199):\n",
    "            env.render()  # Render the current state of the environment\n",
    "            action = policy[observation]  # Select an action based on the policy\n",
    "            # Perform the action in the environment and get the next state and reward\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Break the loop if the episode is terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                print(\"****************************************************\")\n",
    "                print(f\"EPISODE {episode + 1}\")\n",
    "                print(\"Number of steps:\", step)\n",
    "                break\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the 8x8 version of the FrozenLake environment without rendering for training\n",
    "    env = gym.make('FrozenLake8x8-v1', render_mode=None)\n",
    "    policy = run_value_iteration(env)  # Run value iteration to get the policy\n",
    "\n",
    "    # Test the policy and print the average reward\n",
    "    average_reward = test_policy(env, policy, 100)\n",
    "    print(\"Average Score over time: \" + str(average_reward))\n",
    "\n",
    "    # Create the 8x8 environment with rendering for demonstration\n",
    "    env = gym.make('FrozenLake8x8-v1', render_mode=\"human\")\n",
    "    render_policy(env, policy, 5)  # Render and demonstrate the policy\n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b624072-0113-4135-bf97-e73e10237224",
   "metadata": {},
   "source": [
    "#### q_learning_agent.py\n",
    "- this script defines a q-learning agent, tests and renders it on the 4x4 Frozen Lake environment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec760079-cecc-4358-a7e1-9991569d7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Importing the Gym library for creating and managing environments\n",
    "import numpy as np  # Importing NumPy for numerical operations\n",
    "import random  # Importing random for stochastic elements in Q-Learning\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.8, discount_factor=0.95, exploration_rate=1.0, max_exploration_rate=1.0, min_exploration_rate=0.01, exploration_decay_rate=0.005):\n",
    "        self.env = env  # The environment in which the agent operates\n",
    "        self.learning_rate = learning_rate  # Learning rate for Q-learning updates\n",
    "        self.discount_factor = discount_factor  # Discount factor for future rewards\n",
    "        self.exploration_rate = exploration_rate  # Initial exploration rate\n",
    "        self.max_exploration_rate = max_exploration_rate  # Maximum exploration rate\n",
    "        self.min_exploration_rate = min_exploration_rate  # Minimum exploration rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate  # Rate at which exploration rate decays\n",
    "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))  # Initialize Q-table with zeros\n",
    "\n",
    "    def train(self, total_episodes, max_steps):\n",
    "        rewards = []  # To store rewards obtained in each episode\n",
    "\n",
    "        # Training loop over episodes\n",
    "        for episode in range(total_episodes):\n",
    "            state, _ = self.env.reset()  # Reset the environment\n",
    "            total_rewards = 0  # Initialize total rewards for the episode\n",
    "\n",
    "            # Loop for each step in an episode\n",
    "            for step in range(max_steps):\n",
    "                exp_exp_tradeoff = random.uniform(0, 1)  # Exploration-exploitation decision\n",
    "                # Choose action based on exploration rate or Q-table\n",
    "                if exp_exp_tradeoff > self.exploration_rate:\n",
    "                    action = np.argmax(self.q_table[state, :])  # Exploitation (choosing best action)\n",
    "                else:\n",
    "                    action = self.env.action_space.sample()  # Exploration (choosing random action)\n",
    "\n",
    "                # Perform action and get new state and reward\n",
    "                new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Update Q-table using the Q-learning algorithm\n",
    "                self.q_table[state, action] = self.q_table[state, action] + \\\n",
    "                    self.learning_rate * (reward + self.discount_factor * np.max(self.q_table[new_state, :]) - self.q_table[state, action])\n",
    "\n",
    "                total_rewards += reward  # Update total rewards\n",
    "                state = new_state  # Update state\n",
    "\n",
    "                # Break if the episode has ended\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            # Adjust the exploration rate\n",
    "            self.exploration_rate = self.min_exploration_rate + \\\n",
    "                (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate * episode)\n",
    "            rewards.append(total_rewards)  # Store rewards for this episode\n",
    "\n",
    "        print(\"Score over time: \" + str(sum(rewards) / total_episodes))  # Print average reward\n",
    "\n",
    "    def test(self, total_episodes):\n",
    "        # Testing loop over episodes\n",
    "        for episode in range(total_episodes):\n",
    "            state, _ = self.env.reset()  # Reset the environment\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            # Loop until the episode ends\n",
    "            while not done:\n",
    "                action = self.get_action(state)  # Choose action based on Q-table\n",
    "                state, reward, terminated, truncated, _ = self.env.step(action)  # Perform action\n",
    "                total_reward += reward  # Update total reward\n",
    "                done = terminated or truncated  # Check if episode ended\n",
    "                self.env.render()  # Render the environment\n",
    "\n",
    "            print(f\"Episode {episode+1}: Total Reward: {total_reward}\")  # Print total reward for the episode\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return np.argmax(self.q_table[state, :])  # Choose the best action based on Q-table\n",
    "\n",
    "    def play(self, total_episodes, max_steps):\n",
    "        # Play loop over episodes\n",
    "        for episode in range(total_episodes):\n",
    "            state, _ = self.env.reset()  # Reset the environment\n",
    "            print(\"****************************************************\")\n",
    "            print(\"EPISODE \", episode)\n",
    "\n",
    "            # Loop for each step in an episode\n",
    "            for step in range(max_steps):\n",
    "                action = np.argmax(self.q_table[state, :])  # Choose the best action based on Q-table\n",
    "                new_state, reward, terminated, truncated, _ = self.env.step(action)  # Perform action\n",
    "\n",
    "                done = terminated or truncated  # Check if episode ended\n",
    "                if done:\n",
    "                    self.env.render()  # Render the environment\n",
    "                    print(\"Number of steps\", step)  # Print number of steps taken\n",
    "                    break\n",
    "                state = new_state  # Update state\n",
    "\n",
    "        self.env.close()  # Close the environment\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('FrozenLake-v1', render_mode=None)  # Create environment without rendering for training\n",
    "    agent = QLearningAgent(env)  # Initialize QLearningAgent\n",
    "    agent.train(15000, 99)  # Train the agent\n",
    "    env.close()  # Close the environment\n",
    "\n",
    "    env = gym.make('FrozenLake-v1', render_mode=\"human\")  # Create environment with rendering for playing\n",
    "    agent.env = env  # Update the agent's environment\n",
    "    agent.play(5, 99)  # Play the game using the trained Q-table\n",
    "    env.close()  # Close the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc9905-ac5b-4bcb-b501-094af4122e49",
   "metadata": {},
   "source": [
    "#### q_learning_agent_flv2.py\n",
    "- this script tests a q-learning agent on the 8x8 version of Frozen Lake. It's only changes from the above q_learning_agent.py file are in the execution block, code specified below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375900fa-335a-4b46-8617-ab2470ef5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('FrozenLake8x8-v1', render_mode=None)\n",
    "    agent = QLearningAgent(env)\n",
    "    agent.train(50000, 399)  # Updated number of training episodes and max_steps\n",
    "    env.close()\n",
    "\n",
    "    env = gym.make('FrozenLake8x8-v1', render_mode=\"human\")\n",
    "    agent.env = env\n",
    "    agent.play(5, 399)  # Using the Q-table as a 'cheatsheet' to play\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa0b83-adbb-4987-9e43-abd95141ed86",
   "metadata": {},
   "source": [
    "#### dql_agent.py\n",
    "- this script creates a deep q-learning neural network, trains and renders it for the standard 4x4 Frozen Lake: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf2f8d-8212-43a0-acf2-fd105cfb6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Importing the Gym library for creating and managing environments\n",
    "import numpy as np  # Importing NumPy for numerical operations\n",
    "import random  # Importing random for stochastic elements in Deep Q-Learning\n",
    "import tensorflow as tf  # Importing TensorFlow for building neural network\n",
    "from tensorflow.keras.models import Sequential  # Sequential model for creating neural network\n",
    "from tensorflow.keras.layers import Dense  # Dense layer for neural network\n",
    "from tensorflow.keras.optimizers import Adam  # Adam optimizer for training neural network\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, learning_rate=0.001, discount_factor=0.95, exploration_rate=1.0, max_exploration_rate=1.0, min_exploration_rate=0.01, exploration_decay_rate=0.005):\n",
    "        self.env = env  # The environment in which the agent operates\n",
    "        self.learning_rate = learning_rate  # Learning rate for neural network\n",
    "        self.discount_factor = discount_factor  # Discount factor for future rewards\n",
    "        self.exploration_rate = exploration_rate  # Initial exploration rate\n",
    "        self.max_exploration_rate = max_exploration_rate  # Maximum exploration rate\n",
    "        self.min_exploration_rate = min_exploration_rate  # Minimum exploration rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate  # Rate at which exploration rate decays\n",
    "\n",
    "        # Neural Network model for Deep Q-Learning\n",
    "        self.model = Sequential([\n",
    "            Dense(24, input_shape=(env.observation_space.n,), activation='relu'),  # First hidden layer\n",
    "            Dense(24, activation='relu'),  # Second hidden layer\n",
    "            Dense(env.action_space.n, activation='linear')  # Output layer\n",
    "        ])\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))  # Compile the model\n",
    "\n",
    "    def train(self, total_episodes, max_steps):\n",
    "        # Training loop over episodes\n",
    "        for episode in range(total_episodes):\n",
    "            state, _ = self.env.reset()  # Reset the environment\n",
    "            state_one_hot = np.identity(self.env.observation_space.n)[state:state+1]  # One-hot encode state\n",
    "            total_rewards = 0  # Initialize total rewards for the episode\n",
    "\n",
    "            # Loop for each step in an episode\n",
    "            for step in range(max_steps):\n",
    "                # Exploration-exploitation decision\n",
    "                if random.uniform(0, 1) > self.exploration_rate:\n",
    "                    action = np.argmax(self.model.predict(state_one_hot)[0])  # Exploitation\n",
    "                else:\n",
    "                    action = self.env.action_space.sample()  # Exploration\n",
    "\n",
    "                # Perform action and get new state and reward\n",
    "                new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                new_state_one_hot = np.identity(self.env.observation_space.n)[new_state:new_state+1]  # One-hot encode new state\n",
    "\n",
    "                # Update target for Q-value\n",
    "                target = (reward + self.discount_factor * \n",
    "                        np.max(self.model.predict(new_state_one_hot)[0]))\n",
    "                target_f = self.model.predict(state_one_hot)\n",
    "                target_f[0][action] = target\n",
    "\n",
    "                # Fit the model\n",
    "                self.model.fit(state_one_hot, target_f, epochs=1, verbose=0)\n",
    "                total_rewards += reward  # Update total rewards\n",
    "                state_one_hot = new_state_one_hot  # Update state\n",
    "\n",
    "                # Break if the episode has ended\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            # Adjust the exploration rate\n",
    "            self.exploration_rate = self.min_exploration_rate + \\\n",
    "                (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate * episode)\n",
    "\n",
    "            print(f\"Episode {episode+1}: Total Reward: {total_rewards}\")  # Print reward for the episode\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state_one_hot = np.identity(self.env.observation_space.n)[state:state+1]  # One-hot encode state\n",
    "        return np.argmax(self.model.predict(state_one_hot)[0])  # Choose action based on Q-values\n",
    "\n",
    "    def play(self, total_episodes):\n",
    "        # Play loop over episodes\n",
    "        for episode in range(total_episodes):\n",
    "            state, _ = self.env.reset()  # Reset the environment\n",
    "            state_one_hot = np.identity(self.env.observation_space.n)[state:state+1]  # One-hot encode state\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            print(\"****************************************************\")\n",
    "            print(f\"EPISODE {episode + 1}\")\n",
    "\n",
    "            while not done:\n",
    "                self.env.render()  # Render the environment\n",
    "                action = np.argmax(self.model.predict(state_one_hot)[0])  # Choose action based on Q-values\n",
    "                new_state, reward, terminated, truncated, _ = self.env.step(action)  # Perform action\n",
    "                new_state_one_hot = np.identity(self.env.observation_space.n)[new_state:new_state+1]  # One-hot encode new state\n",
    "\n",
    "                state_one_hot = new_state_one_hot  # Update state\n",
    "                done = terminated or truncated  # Check if episode ended\n",
    "                step += 1\n",
    "\n",
    "            print(f\"Episode {episode + 1} finished after {step} steps\")  # Print steps taken\n",
    "\n",
    "        self.env.close()  # Close the environment\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('FrozenLake-v1', render_mode=None)  # Create environment without rendering for training\n",
    "    agent = DQNAgent(env)  # Initialize DQNAgent\n",
    "    agent.train(5000, 99)  # Train the agent\n",
    "    env.close()  # Close the environment\n",
    "\n",
    "    env = gym.make('FrozenLake-v1', render_mode=\"human\")  # Create environment with rendering for playing\n",
    "    agent.env = env  # Update the agent's environment\n",
    "    agent.play(5)  # Play 5 episodes in human mode\n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbad076b-78f5-4b69-88b7-a125064e124a",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "These results are obtained from screenshotted images of the output terminal after running the scripts above. The hyperparameters of all models (particularly the numbers of training episodes and steps/episode) were tweaked and changed in various ways - the most optimal results of all hyperparameter optimizations are shown below: \n",
    "\n",
    "### Value Iteration Agent Results\n",
    "\n",
    "#### 4x4 \n",
    "\n",
    "![Value Iteration 4x4 Results](./images/value_it_4x4_results.PNG)\n",
    "\n",
    "From the average score over time (100 episodes of training), we see that the agent finds the reward and exits 73% of the time (because of the stochastic nature of the environment, this may change with repeated runs of the script). We test the script on 5 different episodes after training and visualize it within the human render mode of the environment - the terminal then outputs the number of steps run in each episode. We can see that there is a large range of steps taken per episode, even when the agent is returning successful most of the time. (Note: The demo video shows different episodes generated from a different run of the script, and will not match the image above)\n",
    "\n",
    "#### 8x8 \n",
    "\n",
    "![Value Iteration 8x8 Results](./images/value_it_8x8_results.PNG)\n",
    "\n",
    "Again, this shows the average score over time over 100 episodes of training, so the agent finds the reward for 77 of the 100 training episodes. This is surprisingly, even better than the 4x4 map environment. Again, we see a wide range of steps/episode in the 5 visualized for testing purposes - the episodes with small step numbers (17 and 21) were instances that the agent fell into a hole before finding the reward. \n",
    "\n",
    "### Q-Learning Agent Results\n",
    "\n",
    "#### 4x4 \n",
    "\n",
    "![Q-learning 4x4](./images/q_learning_4x4_results.PNG)\n",
    "\n",
    "The Q-learning agent shows success in finding the reward and exiting a little less than half the time - however, this is after training the agent for 15000 episodes - a remarkably worse success rate requiring tens of thousands of more episodes of training than the value iteration agent. The range in the number of steps per episode is much less, however, and the video demonstration of the q-learning agent runs much faster than the value iteration agent. \n",
    "\n",
    "#### 8x8 \n",
    "\n",
    "![Q-learning 8x8](./images/q_learning_8x8_results.PNG)\n",
    "\n",
    "This model was trained on larger and larger numbers of training episodes, taking longer and longer times to train, until 50000 episodes with 399 steps/episode, with a 0% chance of success for the model. We can see the number of steps is the same every time, and the visualization of this model shows the agent traveling only vertically in one column of the map. Reasons for the failure of this model to find success are discussed in the Conclusion section below. \n",
    "\n",
    "### DQL Results\n",
    "\n",
    "![Frozen Lake DQL](./images/dql_results.PNG)\n",
    "\n",
    "This model was trained with 5000 episodes - due to limits of computational resources and training time (vastly increased, even for a very simple neural network as was defined for this model), no further numbers of training episodes were tested. The image above shows a snapshot of training after 100 episodes, every epoch of the network coming up with a 0% chance of success with no improvement. This continued for 5000 episodes, and this approach, unfortunately, also did not yield success for the DQL agent. Reasons for the failure of this model to find success are discussed in the Conclusion section below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b11816-8666-485d-8756-51c44844fadd",
   "metadata": {},
   "source": [
    "# Conclusion, Discussion, and Reflection\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "The project's exploration of reinforcement learning models in the FrozenLake environment yielded insightful results. The performance of Value Iteration, Q-Learning, and Deep Q-Learning Network (DQN) models varied significantly across the 4x4 and 8x8 maps.\n",
    "\n",
    "### Value Iteration Results\n",
    "\n",
    "- **4x4 Map**: Achieved an average score of 0.73 over 100 episodes. Steps per episode varied, with a sample range of 37 to 70 steps.\n",
    "- **8x8 Map**: Surprisingly outperformed its 4x4 counterpart with an average score of 0.77, although with a larger spread in steps per episode (range from 17 to 136).\n",
    "\n",
    "### Q-Learning Results\n",
    "\n",
    "- **4x4 Map**: Had a moderate average score of 0.49053333, but exhibited faster execution and a narrower spread of steps per episode (range from 14 to 40).\n",
    "- **8x8 Map**: Showed no success, with an average score of 0.0 after 50,000 training episodes, typically reaching 199 steps per episode.\n",
    "\n",
    "### Deep Q-Learning Results\n",
    "\n",
    "- The 4x4 map resulted in an average reward of 0 after extensive training (5,000 episodes).\n",
    "\n",
    "## Analysis of Results\n",
    "\n",
    "### Value Iteration Performance Analysis\n",
    "\n",
    "#### 4x4 Map Performance\n",
    "\n",
    "- **Average Score**: The value iteration agent achieved an average score of 0.73 over 100 episodes. This high score indicates a good level of proficiency in navigating the simpler 4x4 environment. \n",
    "- **Steps per Episode**: The observed range of steps per episode (37 to 70) suggests a variability in the efficiency of the generated policy. This could be attributed to the stochastic nature of the environment where the slippery condition can lead to less predictable movements.\n",
    "- **Implementation Aspects**: The setting of the discount factor (0.9) and the convergence threshold (theta=1e-4) in the value iteration algorithm influenced the policy development. A higher discount factor emphasizes the importance of future rewards, leading to a policy that may take more steps but aims for higher overall rewards.\n",
    "\n",
    "#### 8x8 Map Performance\n",
    "\n",
    "- **Average Score Improvement**: Interestingly, the average score improved to 0.77 on the 8x8 map. This could be due to the larger state space allowing for more nuanced policy development, where the value iteration algorithm can better differentiate between good and bad states over a larger scale.\n",
    "- **Greater Steps Variability**: The wider range of steps per episode (17 to 136) on the 8x8 map can be attributed to the increased complexity and the larger number of possible paths to the goal. This reflects the algorithm's ability to explore various strategies in a more complex environment.\n",
    "- **Policy Extraction in Larger State Space**: The policy extraction phase of the value iteration might have benefited from the increased diversity in state transitions available in the 8x8 environment, leading to a more robust policy despite the larger state space.\n",
    "\n",
    "#### Implications and Future Improvements\n",
    "\n",
    "- **Hyperparameter Tuning**: Adjusting the discount factor and convergence threshold specifically for each environment could potentially optimize the performance. A lower discount factor might reduce the variability in steps per episode in the 4x4 environment.\n",
    "- **Exploration of Stochasticity**: Further analysis of the impact of the environment's stochastic nature on policy performance could provide insights for improvement. Experimenting with environments having different levels of stochasticity (in a different environment besides Frozen Lake but with a similar state space and goals) might reveal more about the algorithm's adaptability.\n",
    "- **Incremental Complexity Approach**: Gradually increasing the complexity of the training environment, starting from a simple deterministic version and moving towards more complex and stochastic versions, could help in understanding the scalability of the value iteration algorithm.\n",
    "\n",
    "### Q-Learning Performance Analysis\n",
    "\n",
    "#### 4x4 Map Performance\n",
    "\n",
    "- **Average Score**: The Q-learning agent achieved an average score of 0.49053333 over 15,000 training episodes. This moderate success rate reflects the agent's ability to learn an effective policy in a relatively small state space.\n",
    "- **Exploration-Exploitation Trade-off**: The initial high exploration rate, which decayed over time, allowed the agent to explore various actions in the early stages and gradually focus more on exploitation. This strategy is crucial in environments like FrozenLake, where certain actions can lead to falling in holes, and safe paths need to be learned over time.\n",
    "- **Steps per Episode**: The smaller spread of steps per episode (ranging from 14 to 40) suggests that the Q-learning agent was relatively efficient in navigating the environment, possibly due to the balance between exploration and exploitation achieved through the decay of the exploration rate.\n",
    "- **Learning Rate and Discount Factor**: The relatively high learning rate (0.8) and discount factor (0.95) might have contributed to the agent’s ability to adapt its policy based on the feedback from the environment. \n",
    "\n",
    "#### 8x8 Map Performance\n",
    "\n",
    "- **Lack of Success**: On the larger 8x8 map, the Q-learning agent did not achieve any success (average score over all episodes was 0.0) after 50,000 episodes of training. This indicates a significant challenge in scaling the Q-learning approach to larger, more complex environments.\n",
    "- **Potential Reasons for Poor Performance**:\n",
    "    - **Complexity of State Space**: The increased complexity and size of the 8x8 map may have exceeded the capacity of the agent to learn an effective policy within the given number of episodes. \n",
    "    - **Insufficient Exploration**: Given the larger environment, the agent might have needed more exploration to effectively learn about the diverse states and actions possible, especially in the early stages of training.\n",
    "    - **Hyperparameter Tuning**: The learning rate, discount factor, and exploration decay rate may not have been optimal for the larger environment, requiring further tuning to adapt to the increased complexity.\n",
    "\n",
    "### Implications and Future Improvements\n",
    "\n",
    "- **Enhanced Exploration Techniques**: Implementing advanced exploration techniques, such as epsilon-greedy with a more adaptive decay rate or even sophisticated methods like Upper Confidence Bound (UCB), could improve learning in complex environments.\n",
    "- **Hyperparameter Optimization**: Adjusting the learning rate, discount factor, and exploration decay rate specifically for each environment size could yield better results, especially in larger environments.\n",
    "- **Incremental Complexity**: Training the agent initially on smaller environments and progressively increasing the environment size might help in building a more robust learning process.\n",
    "- **More Computational Resources**: An analysis environment with more computational resources could train more episodes in a shorter time, which may be required for q-learning of the 8x8 map. \n",
    "\n",
    "### Deep Q-Learning (DQN) Agent Analysis\n",
    "\n",
    "- The DQN agent, despite extensive training (5000 episodes), did not achieve success in the 4x4 map of the FrozenLake environment, reflected in an average reward of 0 over all episodes. This result prompts a deeper analysis of the factors influencing the DQN agent's performance.\n",
    "\n",
    "#### Factors Influencing DQN Performance\n",
    "\n",
    "- **Complexity of State Representation**: The DQN agent's neural network utilized a one-hot encoding for the states, which may not be the most effective representation for capturing the nuances of the FrozenLake environment. This method of state representation could limit the network's ability to learn complex policies.\n",
    "- **Neural Network Architecture**: The DQN's model, comprising two hidden layers with 24 neurons each, might not have been sufficiently complex to capture the required policy for navigation. Computational resources of my local machine prevented implementation of an overly complex neural network.\n",
    "- **Learning Rate and Discount Factor**: The chosen learning rate (0.001) and discount factor (0.95) play a crucial role in the learning process. The learning rate may have been too low, slowing down the learning process significantly. In contrast, the discount factor might have been too high for an environment where immediate rewards are sparse.\n",
    "- **Exploration and Exploitation Balance**: The exploration strategy, starting from a high exploration rate and decaying over time, might not have been optimal. This could have led to insufficient exploration or premature convergence to a suboptimal policy.\n",
    "- **Reward Structure Sensitivity**: DQN is known to be sensitive to the reward structure of the environment. In FrozenLake, where the rewards are sparse and only obtained upon reaching the goal, the agent might struggle to associate actions with positive outcomes effectively.\n",
    "\n",
    "### Suggestions for Improvement\n",
    "\n",
    "- **State Representation Enhancement**: Experimenting with different state representation techniques, such as embedding layers or simpler state aggregation methods, could improve the learning efficiency of the network.\n",
    "- **Neural Network Tuning**: Adjusting the neural network's architecture, like varying the number of layers and neurons, could help in finding a more suitable model complexity for the task. A computational network with more resources likely could have helped in this endeavor.\n",
    "- **Hyperparameter Optimization**: Fine-tuning the learning rate and discount factor could accelerate learning or improve the agent's ability to value future rewards appropriately.\n",
    "- **Advanced Exploration Strategies**: Implementing more sophisticated exploration strategies, such as adding noise to actions or using methods like epsilon-greedy with a variable decay rate, could enhance the agent's exploration efficiency.\n",
    "\n",
    "## Reflection\n",
    "\n",
    "This project illuminated the nuances of applying different reinforcement learning models to environments of varying complexity. The surprising performance of value iteration in a more complex environment and the challenges faced by Q-learning and DQN highlight the importance of context-specific model selection and hyperparameter tuning in reinforcement learning. These insights pave the way for future exploration into more complex environments assuming sufficient computational resources can be acquired. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0870e0-7c96-4769-8ed5-54cb28332e04",
   "metadata": {},
   "source": [
    "## References: \n",
    "\n",
    "- “Gymnasium Documentation.” Gymnasium.farama.org, gymnasium.farama.org/environments/toy_text/frozen_lake/.\n",
    "- simoninithomas. “Deep_reinforcement_learning_Course/Q Learning/FrozenLake/Q Learning with FrozenLake_unslippery (Deterministic Version).Ipynb at Master · Simoninithomas/Deep_reinforcement_learning_Course.” GitHub, 2018, github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake_unslippery%20(Deterministic%20version).ipynb. Accessed 19 Dec. 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde567e-9c43-4a1e-933b-4f0da3fa04b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
